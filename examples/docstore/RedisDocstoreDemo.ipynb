{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d1c43-4b7f-4917-939f-a964f6f3dafc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67fa07-1395-4aab-a356-72bdb302f6b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12d766-3ca8-4012-9da2-248be80bb6ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, ServiceContext, LLMPredictor, StorageContext\n",
    "from llama_index import VectorStoreIndex, ListIndex, SimpleKeywordTableIndex\n",
    "from llama_index.composability import ComposableGraph\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index.response.notebook_utils import display_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd9d5f-a601-4097-894e-fe98a0c35a5b",
   "metadata": {},
   "source": [
    "#### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdaf9d-cfbd-4ced-8d4e-6eef8508224d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader('../paul_graham_essay/data')\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae82b55-5c9f-432a-9e06-1fccb6f9fc7f",
   "metadata": {},
   "source": [
    "#### Parse into Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e558a-c29f-44ec-ab33-1f481da1a6ef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "nodes = SimpleNodeParser().get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4c8e1-b2ba-4ea6-a8df-978c2788fedc",
   "metadata": {},
   "source": [
    "#### Add to Docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_HOST = os.getenv('REDIS_HOST', '127.0.0.1')\n",
    "REDIS_PORT = os.getenv('REDIS_PORT', 6379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8b0da-67a8-4653-8cdb-09e39583a2d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore import RedisDocumentStore\n",
    "from llama_index.storage.index_store.redis_index_store import RedisIndexStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e781d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=RedisDocumentStore.from_host_and_port(host=REDIS_HOST, port=REDIS_PORT),\n",
    "    index_store=RedisIndexStore.from_host_and_port(host=REDIS_HOST, port=REDIS_PORT),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b18789",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528149c1-5bde-4eba-b75a-e8fa1da17d7c",
   "metadata": {},
   "source": [
    "#### Define Multiple Indexes\n",
    "\n",
    "Each index uses the same underlying Node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fb6ac-2031-4d17-9999-ffdb827f46d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_index = ListIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440f405-fa75-4788-bc7c-11d021a0a17b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ef89f-4ba2-4b1a-b5e5-619e0e8420ef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "keyword_table_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6b2141-fc77-4dec-891b-d4dad0633b35",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-26T15:02:41.461474Z",
     "start_time": "2023-06-26T15:02:41.337117Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'deocode'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# NOTE: the docstore still has the same nodes\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mlen\u001B[39m(\u001B[43mstorage_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdocstore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdocs\u001B[49m)\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/storage/docstore/keyval_docstore.py:65\u001B[0m, in \u001B[0;36mKVDocumentStore.docs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdocs\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, BaseDocument]:\n\u001B[1;32m     59\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get all documents.\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;124;03m        Dict[str, BaseDocument]: documents\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \n\u001B[1;32m     64\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m     json_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_kvstore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcollection\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_node_collection\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {key: json_to_doc(json) \u001B[38;5;28;01mfor\u001B[39;00m key, json \u001B[38;5;129;01min\u001B[39;00m json_dict\u001B[38;5;241m.\u001B[39mitems()}\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/storage/kvstore/redis_kvstore.py:80\u001B[0m, in \u001B[0;36mRedisKVStore.get_all\u001B[0;34m(self, collection)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, val_str \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_redis_client\u001B[38;5;241m.\u001B[39mhscan_iter(name\u001B[38;5;241m=\u001B[39mcollection):\n\u001B[1;32m     79\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(json\u001B[38;5;241m.\u001B[39mloads(val_str))\n\u001B[0;32m---> 80\u001B[0m     collection_kv_dict[\u001B[38;5;28mstr\u001B[39m(\u001B[43mkey\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeocode\u001B[49m())] \u001B[38;5;241m=\u001B[39m value\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m collection_kv_dict\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'bytes' object has no attribute 'deocode'"
     ]
    }
   ],
   "source": [
    "# NOTE: the docstore still has the same nodes\n",
    "len(storage_context.docstore.docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a025b",
   "metadata": {},
   "source": [
    "#### Test out saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b359a08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T15:02:43.659294Z",
     "start_time": "2023-06-26T15:02:43.606383Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: docstore and index_store is persisted in MongoDB by default\n",
    "# NOTE: here only need to persist simple vector store to disk\n",
    "storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b3d2f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T15:02:48.342282Z",
     "start_time": "2023-06-26T15:02:48.180258Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# note down index IDs\u001B[39;00m\n\u001B[1;32m      2\u001B[0m list_id \u001B[38;5;241m=\u001B[39m list_index\u001B[38;5;241m.\u001B[39mindex_id\n\u001B[0;32m----> 3\u001B[0m vector_id \u001B[38;5;241m=\u001B[39m \u001B[43mvector_index\u001B[49m\u001B[38;5;241m.\u001B[39mindex_id\n\u001B[1;32m      4\u001B[0m keyword_id \u001B[38;5;241m=\u001B[39m keyword_table_index\u001B[38;5;241m.\u001B[39mindex_id\n",
      "\u001B[0;31mNameError\u001B[0m: name 'vector_index' is not defined"
     ]
    }
   ],
   "source": [
    "# note down index IDs\n",
    "list_id = list_index.index_id\n",
    "vector_id = vector_index.index_id\n",
    "keyword_id = keyword_table_index.index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1593ca1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T15:02:49.712732Z",
     "start_time": "2023-06-26T15:02:49.476875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading indices with ids: ['bf9196e3-4abf-48b8-b6d3-19ff8236a73a']\n",
      "Loading indices with ids: ['bf9196e3-4abf-48b8-b6d3-19ff8236a73a']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vector_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# load indices\u001B[39;00m\n\u001B[1;32m     10\u001B[0m list_index \u001B[38;5;241m=\u001B[39m load_index_from_storage(storage_context\u001B[38;5;241m=\u001B[39mstorage_context, index_id\u001B[38;5;241m=\u001B[39mlist_id)\n\u001B[0;32m---> 11\u001B[0m vector_index \u001B[38;5;241m=\u001B[39m load_index_from_storage(storage_context\u001B[38;5;241m=\u001B[39mstorage_context, vector_id\u001B[38;5;241m=\u001B[39m\u001B[43mvector_id\u001B[49m)\n\u001B[1;32m     12\u001B[0m keyword_table_index \u001B[38;5;241m=\u001B[39m load_index_from_storage(storage_context\u001B[38;5;241m=\u001B[39mstorage_context, keyword_id\u001B[38;5;241m=\u001B[39mkeyword_id)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'vector_id' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.loading import load_index_from_storage\n",
    "\n",
    "# re-create storage context\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=RedisDocumentStore.from_host_and_port(host=REDIS_HOST, port=REDIS_PORT),\n",
    "    index_store=RedisIndexStore.from_host_and_port(host=REDIS_HOST, port=REDIS_PORT),\n",
    ")\n",
    "\n",
    "# load indices\n",
    "list_index = load_index_from_storage(storage_context=storage_context, index_id=list_id)\n",
    "vector_index = load_index_from_storage(storage_context=storage_context, vector_id=vector_id)\n",
    "keyword_table_index = load_index_from_storage(storage_context=storage_context, keyword_id=keyword_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf6aaf-3375-4212-8323-777969a918f7",
   "metadata": {},
   "source": [
    "#### Test out some Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bba68f3-2743-437e-93b6-ce9ba92e40c3",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-26T15:02:55.782981Z",
     "start_time": "2023-06-26T15:02:55.699883Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_predictor_chatgpt = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context_chatgpt = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt, chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544c0565-72a0-434b-98e5-83138ebdaa2b",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-06-26T15:03:15.143653Z",
     "start_time": "2023-06-26T15:02:56.183558Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m list_index\u001B[38;5;241m.\u001B[39mas_query_engine()\n\u001B[0;32m----> 2\u001B[0m list_response \u001B[38;5;241m=\u001B[39m \u001B[43mquery_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is a summary of this document?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/indices/query/base.py:23\u001B[0m, in \u001B[0;36mBaseQueryEngine.query\u001B[0;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     22\u001B[0m     str_or_query_bundle \u001B[38;5;241m=\u001B[39m QueryBundle(str_or_query_bundle)\n\u001B[0;32m---> 23\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstr_or_query_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/query_engine/retriever_query_engine.py:149\u001B[0m, in \u001B[0;36mRetrieverQueryEngine._query\u001B[0;34m(self, query_bundle)\u001B[0m\n\u001B[1;32m    142\u001B[0m nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retriever\u001B[38;5;241m.\u001B[39mretrieve(query_bundle)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_event_end(\n\u001B[1;32m    144\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mRETRIEVE,\n\u001B[1;32m    145\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mNODES: nodes},\n\u001B[1;32m    146\u001B[0m     event_id\u001B[38;5;241m=\u001B[39mretrieve_id,\n\u001B[1;32m    147\u001B[0m )\n\u001B[0;32m--> 149\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_response_synthesizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynthesize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_bundle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_bundle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_event_end(\n\u001B[1;32m    155\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mQUERY,\n\u001B[1;32m    156\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: response},\n\u001B[1;32m    157\u001B[0m     event_id\u001B[38;5;241m=\u001B[39mquery_id,\n\u001B[1;32m    158\u001B[0m )\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/indices/query/response_synthesis.py:178\u001B[0m, in \u001B[0;36mResponseSynthesizer.synthesize\u001B[0;34m(self, query_bundle, nodes, additional_source_nodes)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_mode \u001B[38;5;241m!=\u001B[39m ResponseMode\u001B[38;5;241m.\u001B[39mNO_TEXT:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_builder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 178\u001B[0m     response_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_response_builder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_bundle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext_chunks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_chunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_response_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    184\u001B[0m     response_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/indices/response/compact_and_refine.py:48\u001B[0m, in \u001B[0;36mCompactAndRefine.get_response\u001B[0;34m(self, query_str, text_chunks, **response_kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m max_prompt \u001B[38;5;241m=\u001B[39m get_biggest_prompt([text_qa_template, refine_template])\n\u001B[1;32m     47\u001B[0m new_texts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_service_context\u001B[38;5;241m.\u001B[39mprompt_helper\u001B[38;5;241m.\u001B[39mrepack(max_prompt, text_chunks)\n\u001B[0;32m---> 48\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_chunks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_texts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kwargs\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/token_counter/token_counter.py:78\u001B[0m, in \u001B[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001B[0;34m(_self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_llm_predict\u001B[39m(_self: Any, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m wrapper_logic(_self):\n\u001B[0;32m---> 78\u001B[0m         f_return_val \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f_return_val\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/indices/response/refine.py:52\u001B[0m, in \u001B[0;36mRefine.get_response\u001B[0;34m(self, query_str, text_chunks, **response_kwargs)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text_chunk \u001B[38;5;129;01min\u001B[39;00m text_chunks:\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m prev_response_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;66;03m# if this is the first chunk, and text chunk already\u001B[39;00m\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;66;03m# is an answer, then return it\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_give_response_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m            \u001B[49m\u001B[43mquery_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtext_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     57\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_refine_response_single(\n\u001B[1;32m     58\u001B[0m             prev_response_obj, query_str, text_chunk\n\u001B[1;32m     59\u001B[0m         )\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/indices/response/refine.py:86\u001B[0m, in \u001B[0;36mRefine._give_response_single\u001B[0;34m(self, query_str, text_chunk, **response_kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cur_text_chunk \u001B[38;5;129;01min\u001B[39;00m text_chunks:\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_streaming:\n\u001B[1;32m     83\u001B[0m         (\n\u001B[1;32m     84\u001B[0m             response,\n\u001B[1;32m     85\u001B[0m             formatted_prompt,\n\u001B[0;32m---> 86\u001B[0m         ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_service_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_predictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtext_qa_template\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcontext_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcur_text_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_prompt_and_response(\n\u001B[1;32m     91\u001B[0m             formatted_prompt, response, log_prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitial\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     92\u001B[0m         )\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_streaming:\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/llm_predictor/base.py:239\u001B[0m, in \u001B[0;36mLLMPredictor.predict\u001B[0;34m(self, prompt, **prompt_args)\u001B[0m\n\u001B[1;32m    234\u001B[0m event_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mon_event_start(\n\u001B[1;32m    235\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mLLM,\n\u001B[1;32m    236\u001B[0m     payload\u001B[38;5;241m=\u001B[39mllm_payload,\n\u001B[1;32m    237\u001B[0m )\n\u001B[1;32m    238\u001B[0m formatted_prompt \u001B[38;5;241m=\u001B[39m prompt\u001B[38;5;241m.\u001B[39mformat(llm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_llm, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprompt_args)\n\u001B[0;32m--> 239\u001B[0m llm_prediction \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprompt_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(llm_prediction)\n\u001B[1;32m    242\u001B[0m \u001B[38;5;66;03m# We assume that the value of formatted_prompt is exactly the thing\u001B[39;00m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;66;03m# eventually sent to OpenAI, or whatever LLM downstream\u001B[39;00m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/llm_predictor/base.py:207\u001B[0m, in \u001B[0;36mLLMPredictor._predict\u001B[0;34m(self, prompt, **prompt_args)\u001B[0m\n\u001B[1;32m    205\u001B[0m full_prompt_args \u001B[38;5;241m=\u001B[39m prompt\u001B[38;5;241m.\u001B[39mget_full_format_args(prompt_args)\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_on_throttling:\n\u001B[0;32m--> 207\u001B[0m     llm_prediction \u001B[38;5;241m=\u001B[39m \u001B[43mretry_on_exceptions_with_backoff\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfull_prompt_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m            \u001B[49m\u001B[43mErrorToRetry\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRateLimitError\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m            \u001B[49m\u001B[43mErrorToRetry\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mServiceUnavailableError\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m            \u001B[49m\u001B[43mErrorToRetry\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTryAgain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m            \u001B[49m\u001B[43mErrorToRetry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m                \u001B[49m\u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAPIConnectionError\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshould_retry\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    219\u001B[0m     llm_prediction \u001B[38;5;241m=\u001B[39m llm_chain\u001B[38;5;241m.\u001B[39mpredict(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfull_prompt_args)\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/utils.py:178\u001B[0m, in \u001B[0;36mretry_on_exceptions_with_backoff\u001B[0;34m(lambda_fn, errors_to_retry, max_tries, min_backoff_secs, max_backoff_secs)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 178\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlambda_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exception_class_tuples \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    180\u001B[0m         traceback\u001B[38;5;241m.\u001B[39mprint_exc()\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/llama_index/llm_predictor/base.py:208\u001B[0m, in \u001B[0;36mLLMPredictor._predict.<locals>.<lambda>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    205\u001B[0m full_prompt_args \u001B[38;5;241m=\u001B[39m prompt\u001B[38;5;241m.\u001B[39mget_full_format_args(prompt_args)\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_on_throttling:\n\u001B[1;32m    207\u001B[0m     llm_prediction \u001B[38;5;241m=\u001B[39m retry_on_exceptions_with_backoff(\n\u001B[0;32m--> 208\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfull_prompt_args\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    209\u001B[0m         [\n\u001B[1;32m    210\u001B[0m             ErrorToRetry(openai\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mRateLimitError),\n\u001B[1;32m    211\u001B[0m             ErrorToRetry(openai\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mServiceUnavailableError),\n\u001B[1;32m    212\u001B[0m             ErrorToRetry(openai\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mTryAgain),\n\u001B[1;32m    213\u001B[0m             ErrorToRetry(\n\u001B[1;32m    214\u001B[0m                 openai\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mAPIConnectionError, \u001B[38;5;28;01mlambda\u001B[39;00m e: e\u001B[38;5;241m.\u001B[39mshould_retry\n\u001B[1;32m    215\u001B[0m             ),\n\u001B[1;32m    216\u001B[0m         ],\n\u001B[1;32m    217\u001B[0m     )\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    219\u001B[0m     llm_prediction \u001B[38;5;241m=\u001B[39m llm_chain\u001B[38;5;241m.\u001B[39mpredict(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfull_prompt_args)\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:252\u001B[0m, in \u001B[0;36mLLMChain.predict\u001B[0;34m(self, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    238\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \n\u001B[1;32m    240\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001B[39;00m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key]\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/chains/base.py:166\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    168\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    169\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[1;32m    170\u001B[0m )\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001B[0m\n\u001B[1;32m    154\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    155\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    156\u001B[0m     inputs,\n\u001B[1;32m    157\u001B[0m )\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    159\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 160\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     89\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m     90\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     91\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m---> 92\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m    101\u001B[0m prompts, stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_prompts(input_list, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/llms/base.py:140\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    134\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    138\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    139\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/llms/base.py:206\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    205\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n\u001B[0;32m--> 206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    207\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_llm_end(output)\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager:\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/llms/base.py:198\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001B[0m\n\u001B[1;32m    193\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    194\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m), prompts, invocation_params\u001B[38;5;241m=\u001B[39mparams, options\u001B[38;5;241m=\u001B[39moptions\n\u001B[1;32m    195\u001B[0m )\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 198\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    202\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    203\u001B[0m     )\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    205\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:302\u001B[0m, in \u001B[0;36mBaseOpenAI._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    300\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invocation_params\n\u001B[1;32m    301\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m--> 302\u001B[0m sub_prompts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_sub_prompts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    303\u001B[0m choices \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    304\u001B[0m token_usage: Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:390\u001B[0m, in \u001B[0;36mBaseOpenAI.get_sub_prompts\u001B[0;34m(self, params, prompts, stop)\u001B[0m\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(prompts) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    387\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens set to -1 not supported for multiple inputs.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m         )\n\u001B[0;32m--> 390\u001B[0m     params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_tokens_for_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    391\u001B[0m sub_prompts \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    392\u001B[0m     prompts[i : i \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size]\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(prompts), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[1;32m    394\u001B[0m ]\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sub_prompts\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:577\u001B[0m, in \u001B[0;36mBaseOpenAI.max_tokens_for_prompt\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m    563\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_tokens_for_prompt\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m    564\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Calculate the maximum number of tokens possible to generate for a prompt.\u001B[39;00m\n\u001B[1;32m    565\u001B[0m \n\u001B[1;32m    566\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    575\u001B[0m \u001B[38;5;124;03m            max_tokens = openai.max_token_for_prompt(\"Tell me a joke.\")\u001B[39;00m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 577\u001B[0m     num_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_context_size \u001B[38;5;241m-\u001B[39m num_tokens\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/base_language.py:90\u001B[0m, in \u001B[0;36mBaseLanguageModel.get_num_tokens\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_num_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m     89\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get the number of tokens present in the text.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_token_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:494\u001B[0m, in \u001B[0;36mBaseOpenAI.get_token_ids\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    487\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import tiktoken python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is needed in order to calculate get_num_tokens. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install tiktoken`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m     )\n\u001B[0;32m--> 494\u001B[0m enc \u001B[38;5;241m=\u001B[39m \u001B[43mtiktoken\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding_for_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m enc\u001B[38;5;241m.\u001B[39mencode(\n\u001B[1;32m    497\u001B[0m     text,\n\u001B[1;32m    498\u001B[0m     allowed_special\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallowed_special,\n\u001B[1;32m    499\u001B[0m     disallowed_special\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisallowed_special,\n\u001B[1;32m    500\u001B[0m )\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/tiktoken/model.py:75\u001B[0m, in \u001B[0;36mencoding_for_model\u001B[0;34m(model_name)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m encoding_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m     71\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not automatically map \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to a tokeniser. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease use `tiktok.get_encoding` to explicitly get the tokeniser you expect.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     73\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_encoding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoding_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/tiktoken/registry.py:63\u001B[0m, in \u001B[0;36mget_encoding\u001B[0;34m(encoding_name)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown encoding \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mencoding_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     62\u001B[0m constructor \u001B[38;5;241m=\u001B[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001B[0;32m---> 63\u001B[0m enc \u001B[38;5;241m=\u001B[39m Encoding(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[43mconstructor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     64\u001B[0m ENCODINGS[encoding_name] \u001B[38;5;241m=\u001B[39m enc\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m enc\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/tiktoken_ext/openai_public.py:38\u001B[0m, in \u001B[0;36mp50k_base\u001B[0;34m()\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mp50k_base\u001B[39m():\n\u001B[0;32m---> 38\u001B[0m     mergeable_ranks \u001B[38;5;241m=\u001B[39m \u001B[43mload_tiktoken_bpe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mp50k_base\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexplicit_n_vocab\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m50281\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspecial_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: {ENDOFTEXT: \u001B[38;5;241m50256\u001B[39m},\n\u001B[1;32m     47\u001B[0m     }\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/tiktoken/load.py:116\u001B[0m, in \u001B[0;36mload_tiktoken_bpe\u001B[0;34m(tiktoken_bpe_file)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_tiktoken_bpe\u001B[39m(tiktoken_bpe_file: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m# NB: do not add caching to this function\u001B[39;00m\n\u001B[0;32m--> 116\u001B[0m     contents \u001B[38;5;241m=\u001B[39m \u001B[43mread_file_cached\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtiktoken_bpe_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m    118\u001B[0m         base64\u001B[38;5;241m.\u001B[39mb64decode(token): \u001B[38;5;28mint\u001B[39m(rank)\n\u001B[1;32m    119\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m token, rank \u001B[38;5;129;01min\u001B[39;00m (line\u001B[38;5;241m.\u001B[39msplit() \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m contents\u001B[38;5;241m.\u001B[39msplitlines() \u001B[38;5;28;01mif\u001B[39;00m line)\n\u001B[1;32m    120\u001B[0m     }\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/tiktoken/load.py:48\u001B[0m, in \u001B[0;36mread_file_cached\u001B[0;34m(blobpath)\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(cache_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m---> 48\u001B[0m contents \u001B[38;5;241m=\u001B[39m \u001B[43mread_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblobpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(cache_dir, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     51\u001B[0m tmp_filename \u001B[38;5;241m=\u001B[39m cache_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(uuid\u001B[38;5;241m.\u001B[39muuid4()) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.tmp\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/tiktoken/load.py:24\u001B[0m, in \u001B[0;36mread_file\u001B[0;34m(blobpath)\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# avoiding blobfile for public files helps avoid auth issues, like MFA prompts\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblobpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m resp\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/requests/sessions.py:747\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    744\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[0;32m--> 747\u001B[0m     \u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/requests/models.py:899\u001B[0m, in \u001B[0;36mResponse.content\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    897\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    898\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 899\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONTENT_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/requests/models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    815\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 816\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    817\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    818\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/urllib3/response.py:628\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp):\n\u001B[0;32m--> 628\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    630\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m    631\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/urllib3/response.py:567\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    564\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 567\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    569\u001B[0m         flush_decoder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Develop/ai/llama_index/.venv/lib/python3.10/site-packages/urllib3/response.py:533\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m buffer\u001B[38;5;241m.\u001B[39mgetvalue()\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    532\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[0;32m--> 533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:466\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[1;32m    465\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[0;32m--> 466\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    469\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "query_engine = list_index.as_query_engine()\n",
    "list_response = query_engine.query(\"What is a summary of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d250be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_response(list_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036077b7-108e-4026-9628-44c694343460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "vector_response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42229e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_response(vector_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7719c-f663-4edb-a239-d2a8f0a5c091",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = keyword_table_index.as_query_engine()\n",
    "keyword_response = query_engine.query(\"What did the author do after his time at YC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37524641-2632-4a76-8ae6-00f1285256d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_response(keyword_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58018c-3117-4d50-abff-16a1873eda9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
