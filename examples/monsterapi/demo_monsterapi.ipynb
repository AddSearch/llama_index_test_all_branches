{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtkWO7S6syax"
      },
      "source": [
        "#Monster API LLM Integration into LLamaIndex\n",
        "\n",
        "MonsterAPI Hosts wide range of popular LLMs as inference service and this notebook serves as a tutorial about how to use llama-index to access MonsterAPI LLMs.\n",
        "\n",
        "\n",
        "Check us out here: https://monsterapi.ai/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BWCGsW5Kguk"
      },
      "source": [
        "Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sodh4Q5Rsyaz",
        "outputId": "976c974e-17bc-4d29-cd5b-8b4fb0833646"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install llama-index --quiet\n",
        "!python3 -m pip install monsterapi --quiet\n",
        "!python3 -m pip install sentence_transformers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH2M0_AHK0iO"
      },
      "source": [
        "Import required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "scLx69_Wsya0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from llama_index.llms import MonsterLLM\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYvkuOXcKvup"
      },
      "source": [
        "Set Monster API Key env variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OGqeNR4NKutV"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MONSTER_API_KEY\"] = \"{}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RpDpGYDK6Rd"
      },
      "source": [
        "## Basic Usage Pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7pk7gAHK_su"
      },
      "source": [
        "Set the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YEJKmC_LAVLE"
      },
      "outputs": [],
      "source": [
        "model = \"llama2-7b-chat\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Md2J5DeLCmM"
      },
      "source": [
        "Initiate LLM module and call complete method with input prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmjTbnu6sya1",
        "outputId": "7589fde0-fe0b-4e1b-b388-9baded849437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hello! I'm just an AI assistant, here to help you with any questions or tasks you may have. My purpose is to provide helpful and respectful responses that are both safe and positive in nature. I strive to be socially unbiased and free from harmful content, ensuring that my answers promote inclusivity and diversity. If a question does not make sense or is factually incorrect, I will explain why instead of providing false information. Please feel free to ask me anything, and I'll do my best to assist you!\n"
          ]
        }
      ],
      "source": [
        "llm = MonsterLLM(model = model, temperature = 0.75)\n",
        "result = llm.complete(\"Who are you?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLEAtjEOsya1"
      },
      "source": [
        "##RAG Approach to import external knowledge into LLM as context\n",
        "\n",
        "Source Paper: https://arxiv.org/pdf/2005.11401.pdf\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a method that uses a combination of pre-defined rules or parameters (non-parametric memory) and external information from the internet (parametric memory) to generate responses to questions or create new ones. By lever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5QjemVOLjcS"
      },
      "source": [
        "Install pypdf library needed to install pdf parsing library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GcJ8ReIg3_sE"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install pypdf --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn-kCKRiLoKK"
      },
      "source": [
        "Lets try to augment our LLM with RAG source paper PDF as external information.\n",
        "Lets download the pdf into data dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ-OoCBp4-Uh",
        "outputId": "ba894d84-9d56-4353-82f9-3f6a20e9fd88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  864k  100  864k    0     0   598k      0  0:00:01  0:00:01 --:--:--  599k\n"
          ]
        }
      ],
      "source": [
        "!rm -r ./data&&mkdir -p data&&cd data&&curl 'https://arxiv.org/pdf/2005.11401.pdf' -o \"RAG.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrxV-SitMTLx"
      },
      "source": [
        "Load the document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "M4S32c8-sya1"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDm16rWMMFGI"
      },
      "source": [
        "Initiate LLM and Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "obHXuo5t8l7H"
      },
      "outputs": [],
      "source": [
        "llm = MonsterLLM(model = model, temperature = 0.75, context_window=1024\n",
        ")\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024, llm=llm, embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI5oMOCFMa96"
      },
      "source": [
        "Create embedding store and create index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YwvPwGD59w5f"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atDN3bHNMfNw"
      },
      "source": [
        "Actual LLM output without RAG:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etSwpAwD-Qax",
        "outputId": "11d26ab0-a3ba-4536-ca94-b3bf64b74ad2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletionResponse(text=\" Thank you for asking! Retrieval-Augmented Generation (RAG) is a machine learning technique that combines the strengths of two popular AI models: Generative Models and Language Retrieval Systems.\\nGenerative Models, such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs), are designed to generate new, synthetic data that resembles existing examples. In the context of text generation, these models can produce coherent and often realistic sentences or paragraphs. However, they may struggle with generating novel ideas or exploring complex topics beyond what they have been trained on.\\nLanguage Retrieval Systems, on the other hand, are designed to retrieve relevant information from a large dataset given a query or prompt. These systems use various techniques like keyword extraction, entity recognition, and semantic search to identify the most relevant passages or sentences in response to a user's request. While language retrieval systems are excellent at surfacing specific information within a known domain, they may not be able to generate entirely new content or explore creative possibilities outside of their training datasets.\\nRAG addresses this limitation by combining both generative and retrieval capabilities into one model\", additional_kwargs={}, raw=None, delta=None)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.complete(\"What is Retrieval-Augmented Generation?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcBop8OCMlcr"
      },
      "source": [
        "LLM Output with RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBdqZErI-P8G",
        "outputId": "180ec98a-3cb1-4c9b-f973-a068ba40a00b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Thank you for providing additional context! Based on the information provided, I can refine the original answer to better address your question. Here's my revised response:\n",
            "Retrieval-Augmented Generation (RAG) is a technique used in Natural Language Processing (NLP) that leverages pre-trained language models like BART and FAISS to improve the quality of generated text. The basic idea behind RAG is to use a parameterized memory component, such as BART, to store and retrieve knowledge from a large corpus of text, which can then be utilized to generate new text that is similar in style and content to the original training data.\n",
            "In more detail, when generating text using RAG, the model first generates incomplete or partial sentences or phrases, and then uses the stored knowledge in the parameterized memory component to complete them. This process helps guide the generation, drawing out specific knowledge stored in the parametric memory, resulting in more accurate and informative responses.\n",
            "RAG has several advantages over other NLP techniques. Firstly, it allows for faster and more efficient generation by reducing the need for expensive and computationally intensive decoding methods. Secondly, it enables more flexible and cre\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What is Retrieval-Augmented Generation?\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
