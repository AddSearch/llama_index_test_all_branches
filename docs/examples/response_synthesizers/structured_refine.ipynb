{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "540ff471-dcea-4b3e-9c0c-a3173f1c640e",
   "metadata": {},
   "source": [
    "# Refine with Structured Answer Filtering\n",
    "When using our Refine response synthesizer for response synthesis, it's crucial to filter out non-answers. An issue often encountered is the propagation of a single unhelpful response like \"I don't have the answer\", which can persist throughout the synthesis process and lead to a final answer of the same nature. This can occur even when there are actual answers present in other, more relevant sections.\n",
    "\n",
    "These unhelpful responses can be filtered out by setting `structured_answer_filtering` to `True`. It is set to `False` by default since this currently only works best if you are using an OpenAI model that supports function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20854a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "158b08a8-32d3-4397-ad37-75870416226b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6b6f5c-5852-41be-8ce8-d94c520e0e50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The president in the year 2040 is John Cena.\",\n",
    "    \"The president in the year 2050 is Florence Pugh.\",\n",
    "    'The president in the year 2060 is Dwayne \"The Rock\" Johnson.',\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efed56ee-fcd3-439c-a1b2-53c643f15c8e",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae4dad4-6044-4c9c-becd-4e2908b54a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65c577c-215e-40e9-8f3f-c23a09af7574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c48278-f5b2-47bb-a240-6b66a191c6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"refine\", service_context=service_context, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834ac725-54ce-4243-bc09-4a50e2590b28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Refine context: The president in the year 2050 is Florence Pugh.\n",
      "> Refine context: The president in the year 2060 is Dwayne \"The R...\n"
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2744b",
   "metadata": {},
   "source": [
    "### Failed Result\n",
    "As you can see, we weren't able to get the correct answer from the input `texts` strings since the initial \"I don't know\" answer propogated through till the end of the response synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a600aa73-74b8-4a20-8f56-1b273417f788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to information about the future.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b85d5",
   "metadata": {},
   "source": [
    "Now we'll try again with `structured_answer_filtering=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27488623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"refine\",\n",
    "    service_context=service_context,\n",
    "    verbose=True,\n",
    "    structured_answer_filtering=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eac8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: StructuredRefineResponse with args: {\n",
      "  \"answer\": \"There is not enough context information to determine who is the president in the year 2050.\",\n",
      "  \"query_satisfied\": false\n",
      "}\n",
      "> Refine context: The president in the year 2050 is Florence Pugh.\n",
      "Function call: StructuredRefineResponse with args: {\n",
      "  \"answer\": \"Florence Pugh\",\n",
      "  \"query_satisfied\": true\n",
      "}\n",
      "> Refine context: The president in the year 2060 is Dwayne \"The R...\n",
      "Function call: StructuredRefineResponse with args: {\n",
      "  \"answer\": \"Florence Pugh\",\n",
      "  \"query_satisfied\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed92fb",
   "metadata": {},
   "source": [
    "### Successful Result\n",
    "As you can see, we were able to determine the correct answer from the given context by filtering the `texts` strings for the ones that actually contained the answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf0503c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florence Pugh\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6668e7",
   "metadata": {},
   "source": [
    "## Non Function-calling LLMs\n",
    "You may want to make use of this filtering functionality with an LLM that doesn't offer a function calling API.\n",
    "\n",
    "In that case, the `Refine` module will automatically switch to using a structured output `Program` that doesn't rely on an external function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f6f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll stick with OpenAI but use an older model that does not support function calling\n",
    "davinci_llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53fddd33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown model: gpt-3.5-turbo-instruct. Please provide a valid OpenAI model name.Known models are: gpt-4, gpt-4-32k, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m ServiceContext\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresponse_synthesizers\u001b[39;00m \u001b[39mimport\u001b[39;00m get_response_synthesizer\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m davinci_service_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults(llm\u001b[39m=\u001b[39;49mdavinci_llm)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m summarizer \u001b[39m=\u001b[39m get_response_synthesizer(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     response_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrefine\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     service_context\u001b[39m=\u001b[39mdavinci_service_context,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     structured_answer_filtering\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/indices/service_context.py:168\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    164\u001b[0m embed_model \u001b[39m=\u001b[39m resolve_embed_model(embed_model)\n\u001b[1;32m    165\u001b[0m embed_model\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[1;32m    167\u001b[0m prompt_helper \u001b[39m=\u001b[39m prompt_helper \u001b[39mor\u001b[39;00m _get_default_prompt_helper(\n\u001b[0;32m--> 168\u001b[0m     llm_metadata\u001b[39m=\u001b[39mllm_predictor\u001b[39m.\u001b[39;49mmetadata,\n\u001b[1;32m    169\u001b[0m     context_window\u001b[39m=\u001b[39mcontext_window,\n\u001b[1;32m    170\u001b[0m     num_output\u001b[39m=\u001b[39mnum_output,\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m node_parser \u001b[39m=\u001b[39m node_parser \u001b[39mor\u001b[39;00m _get_default_node_parser(\n\u001b[1;32m    174\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size,\n\u001b[1;32m    175\u001b[0m     chunk_overlap\u001b[39m=\u001b[39mchunk_overlap,\n\u001b[1;32m    176\u001b[0m     callback_manager\u001b[39m=\u001b[39mcallback_manager,\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    179\u001b[0m llama_logger \u001b[39m=\u001b[39m llama_logger \u001b[39mor\u001b[39;00m LlamaLogger()\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/llm_predictor/base.py:145\u001b[0m, in \u001b[0;36mLLMPredictor.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmetadata\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMMetadata:\n\u001b[1;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get LLM metadata.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mmetadata\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/llms/openai.py:113\u001b[0m, in \u001b[0;36mOpenAI.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmetadata\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMMetadata:\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMMetadata(\n\u001b[0;32m--> 113\u001b[0m         context_window\u001b[39m=\u001b[39mopenai_modelname_to_contextsize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_model_name()),\n\u001b[1;32m    114\u001b[0m         num_output\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    115\u001b[0m         is_chat_model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_chat_model,\n\u001b[1;32m    116\u001b[0m         is_function_calling_model\u001b[39m=\u001b[39mis_function_calling_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_model_name()),\n\u001b[1;32m    117\u001b[0m         model_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m    118\u001b[0m     )\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/llms/openai_utils.py:190\u001b[0m, in \u001b[0;36mopenai_modelname_to_contextsize\u001b[0;34m(modelname)\u001b[0m\n\u001b[1;32m    187\u001b[0m context_size \u001b[39m=\u001b[39m ALL_AVAILABLE_MODELS\u001b[39m.\u001b[39mget(modelname, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m context_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown model: \u001b[39m\u001b[39m{\u001b[39;00mmodelname\u001b[39m}\u001b[39;00m\u001b[39m. Please provide a valid OpenAI model name.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mKnown models are: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(ALL_AVAILABLE_MODELS\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m context_size\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown model: gpt-3.5-turbo-instruct. Please provide a valid OpenAI model name.Known models are: gpt-4, gpt-4-32k, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "davinci_service_context = ServiceContext.from_defaults(llm=davinci_llm)\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"refine\",\n",
    "    service_context=davinci_service_context,\n",
    "    verbose=True,\n",
    "    structured_answer_filtering=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e90911bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not extract json string from output:  It is not possible to answer this question with the given context information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m summarizer\u001b[39m.\u001b[39;49mget_response(\u001b[39m\"\u001b[39;49m\u001b[39mwho is president in the year 2050?\u001b[39;49m\u001b[39m\"\u001b[39;49m, texts)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/loganm/llama_index_proper/llama_index/docs/examples/response_synthesizers/structured_refine.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/response_synthesizers/refine.py:116\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, **response_kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks:\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m prev_response_obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         \u001b[39m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[39m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_give_response_single(\n\u001b[1;32m    117\u001b[0m             query_str,\n\u001b[1;32m    118\u001b[0m             text_chunk,\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m         \u001b[39m# refine response if possible\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refine_response_single(\n\u001b[1;32m    123\u001b[0m             prev_response_obj, query_str, text_chunk\n\u001b[1;32m    124\u001b[0m         )\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/response_synthesizers/refine.py:175\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streaming:\n\u001b[1;32m    173\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m         structured_response \u001b[39m=\u001b[39m cast(\n\u001b[0;32m--> 175\u001b[0m             StructuredRefineResponse, program(context_str\u001b[39m=\u001b[39;49mcur_text_chunk)\n\u001b[1;32m    176\u001b[0m         )\n\u001b[1;32m    177\u001b[0m         query_satisfied \u001b[39m=\u001b[39m structured_response\u001b[39m.\u001b[39mquery_satisfied\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/program/llm_program.py:70\u001b[0m, in \u001b[0;36mLLMTextCompletionProgram.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm\u001b[39m.\u001b[39mcomplete(formatted_prompt)\n\u001b[1;32m     69\u001b[0m raw_output \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[0;32m---> 70\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_parser\u001b[39m.\u001b[39;49mparse(raw_output)\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/output_parsers/pydantic.py:51\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse, validate, and correct errors programmatically.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     json_str \u001b[39m=\u001b[39m extract_json_str(text)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_cls\u001b[39m.\u001b[39mparse_raw(json_str)\n",
      "File \u001b[0;32m~/llama_index_proper/llama_index/llama_index/output_parsers/utils.py:66\u001b[0m, in \u001b[0;36mextract_json_str\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     64\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m{\u001b[39m\u001b[39m.*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m, text\u001b[39m.\u001b[39mstrip(), re\u001b[39m.\u001b[39mMULTILINE \u001b[39m|\u001b[39m re\u001b[39m.\u001b[39mIGNORECASE \u001b[39m|\u001b[39m re\u001b[39m.\u001b[39mDOTALL)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match:\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not extract json string from output: \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m match\u001b[39m.\u001b[39mgroup()\n",
      "\u001b[0;31mValueError\u001b[0m: Could not extract json string from output:  It is not possible to answer this question with the given context information."
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e39730",
   "metadata": {},
   "source": [
    "### `CompactAndRefine`\n",
    "Since `CompactAndRefine` is built on top of `Refine`, this response mode also supports structured answer filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf1c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    service_context=service_context,\n",
    "    verbose=True,\n",
    "    structured_answer_filtering=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbf9213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: StructuredRefineResponse with args: {\n",
      "  \"answer\": \"Florence Pugh\",\n",
      "  \"query_satisfied\": true\n",
      "}\n",
      "Florence Pugh\n"
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index",
   "language": "python",
   "name": "llama-index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
