{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a641c-d90c-4401-ad59-369d70babf5b",
   "metadata": {},
   "source": [
    "# Accessing/Customizing Prompts within Higher-Level Modules\n",
    "\n",
    "LlamaIndex contains a variety of higher-level modules (query engines, response synthesizers, retrievers, etc.), many of which make LLM calls + use prompt templates.\n",
    "\n",
    "This guide shows how you can 1) access the set of prompts for any module (including nested) with `get_prompts`, and 2) update these prompts easily with `update_prompts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c01b6-60ea-457f-8cc9-2a448d401d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94148e-a2f9-44b3-919f-4d9811ae27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352384e6-0f31-4aa9-a351-4d3b278e2afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb6b21-565e-4993-98ab-bad44a2259b9",
   "metadata": {},
   "source": [
    "## Setup: Load Data, Build Index, and Get Query Engine\n",
    "\n",
    "Here we build a vector index over a toy dataset (PG's essay), and access the query engine.\n",
    "\n",
    "The query engine is a simple RAG pipeline consisting of top-k retrieval + LLM synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1a04a-7859-4d97-810d-abde72891d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../../../examples/paul_graham_essay/data\"\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c7e74-c084-40b0-b94b-d454e095e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8534b-eb16-47a7-9269-dfd4ebf97fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d48e1c-15ce-4eb3-9e3c-00e7e547bf2c",
   "metadata": {},
   "source": [
    "## Accessing Prompts\n",
    "\n",
    "Here we get the prompts from the query engine. Note that *all* prompts are returned, including ones used in sub-modules in the query engine. This allows you to centralize a view of these prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28b46d-6115-4095-b1a2-5603a8ee709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d61d36-b2b6-4d04-8284-4004c1004966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:summary_template': 'Context information from multiple sources is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the information from multiple sources and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display({k: p.get_template() for k, p in prompts_dict.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a110a58-2f93-42c5-a1ec-89ecc3692f93",
   "metadata": {},
   "source": [
    "#### Checking `get_prompts` on Response Synthesizer\n",
    "\n",
    "You can also call `get_prompts` on the underlying response synthesizer, where you'll see the same list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d99264-0f53-4da3-aa76-ab88f1425dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary_template': 'Context information from multiple sources is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the information from multiple sources and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.response_synthesizer.get_prompts()\n",
    "display({k: p.get_template() for k, p in prompts_dict.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb003e4-e58b-46dd-abae-fb09b0c891a2",
   "metadata": {},
   "source": [
    "#### Checking `get_prompts` with a different response synthesis strategy\n",
    "\n",
    "Here we try the default `compact` method.\n",
    "\n",
    "We'll see that the set of templates used are different; a QA template and a refine template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523fb00-7106-4849-8ce9-4bbc13dd912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine(response_mode=\"compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27474aef-06c3-4684-8778-9b00ba4ae7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:text_qa_template': 'Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ',\n",
       " 'response_synthesizer:refine_template': \"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "display({k: p.get_template() for k, p in prompts_dict.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545710e-bb29-4dd4-885c-bd9222e67b26",
   "metadata": {},
   "source": [
    "#### Put into query engine, get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936c416-bdd8-48d4-95c3-760b5f2a2bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also mentioned their interest in philosophy and AI.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71103bb5-fe3e-4373-8cf9-b786f64f0189",
   "metadata": {},
   "source": [
    "## Customize the prompt\n",
    "\n",
    "You can also update/customize the prompts with the `update_prompts` function. Pass in arg values with the keys equal to the keys you see in the prompt dictionary.\n",
    "\n",
    "Here we'll change the summary prompt to use Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545df36-4bd5-40d3-b4c8-a01c89774262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "# reset\n",
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "\n",
    "# shakespeare!\n",
    "new_summary_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query in the style of a Shakespeare play.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7687b-bdbf-454e-b3b3-4438b80934a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:summary_template\": new_summary_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad924fca-5074-4a04-99bc-e5dc0102d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e39a0-6db1-419f-816e-c702a8e49e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_synthesizer:text_qa_template': 'Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ',\n",
       " 'response_synthesizer:refine_template': \"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display({k: p.get_template() for k, p in prompts_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b195c32-1913-4d1a-b6cf-3f2635e8773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
