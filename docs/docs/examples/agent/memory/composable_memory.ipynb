{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51784f0-98a9-497a-8e35-257a62c55141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import (\n",
    "    VectorMemory,\n",
    "    SimpleComposableMemory,\n",
    "    ChatMemoryBuffer,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "vector_memory = VectorMemory.from_defaults(\n",
    "    vector_store=None,  # leave as None to use default in-memory vector store\n",
    "    embed_model=OpenAIEmbedding(),\n",
    "    retriever_kwargs={\"similarity_top_k\": 1},\n",
    ")\n",
    "\n",
    "chat_memory_buffer = ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "composable_memory = SimpleComposableMemory.from_defaults(\n",
    "    sources=[chat_memory_buffer, vector_memory]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd78f09-1bbb-4e5c-9535-a222cce9190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "msgs = [\n",
    "    ChatMessage.from_str(\"Jerry likes juice.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Bob likes burgers.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Alice likes apples.\", \"user\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a9b2d-64f0-4d24-a980-94157d906043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into memory\n",
    "for m in msgs:\n",
    "    composable_memory.put(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fc439-9f5b-4560-b06d-e9cb3c078cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='Jerry likes juice.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Bob likes burgers.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Alice likes apples.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve from memory\n",
    "msgs = composable_memory.get(\"What does Jerry like?\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037702b-aef6-4785-982c-2dcdb561c4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a helpful assistant.\\n\\nBelow are a set of relevant dialogues retrieved from potentially several memory sources:\\n\\n=====Relevant messages from memory source 1=====\\n\\n\\tUSER: Jerry likes juice.\\n\\n=====End of relevant messages from memory source 1======\\n\\nThis is the end of the of retrieved message dialogues.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Jerry likes juice.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Bob likes burgers.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Alice likes apples.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = composable_memory.get_and_set(\"What does Jerry like?\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e557e-c6b2-4c80-80e4-b30398aa0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "composable_memory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962c96a-90a8-4913-aede-5fb5ad612758",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [\n",
    "    ChatMessage.from_str(\"Jerry likes burgers.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Bob likes apples.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Indeed, Bob likes apples.\", \"assistant\"),\n",
    "    ChatMessage.from_str(\"Alice likes juice.\", \"user\"),\n",
    "]\n",
    "composable_memory.set(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f073a9-2c4b-457b-9e3f-81f21bf77781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a helpful assistant.\\n\\nBelow are a set of relevant dialogues retrieved from potentially several memory sources:\\n\\n=====Relevant messages from memory source 1=====\\n\\n\\tUSER: Bob likes apples.\\n\\tASSISTANT: Indeed, Bob likes apples.\\n\\n=====End of relevant messages from memory source 1======\\n\\nThis is the end of the of retrieved message dialogues.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Jerry likes burgers.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Bob likes apples.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Indeed, Bob likes apples.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Alice likes juice.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = composable_memory.get_and_set(\"What does Bob like?\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2119c-a747-415c-b8f1-80d8dce84537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n",
      "\n",
      "Below are a set of relevant dialogues retrieved from potentially several memory sources:\n",
      "\n",
      "=====Relevant messages from memory source 1=====\n",
      "\n",
      "\tUSER: Bob likes apples.\n",
      "\tASSISTANT: Indeed, Bob likes apples.\n",
      "\n",
      "=====End of relevant messages from memory source 1======\n",
      "\n",
      "This is the end of the of retrieved message dialogues.\n"
     ]
    }
   ],
   "source": [
    "print(msgs[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79882a-ccc1-4336-87e0-fbcd789cf991",
   "metadata": {},
   "source": [
    "### Plug Memory Module Into Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca5351-91b8-4c7d-a4e7-f2d068a87688",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_memory = VectorMemory.from_defaults(\n",
    "    vector_store=None,  # leave as None to use default in-memory vector store\n",
    "    embed_model=OpenAIEmbedding(),\n",
    "    retriever_kwargs={\"similarity_top_k\": 1},\n",
    ")\n",
    "\n",
    "chat_memory_buffer = ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "composable_memory = SimpleComposableMemory.from_defaults(\n",
    "    sources=[chat_memory_buffer, vector_memory]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d37976-bdb0-45c8-94b8-94df62a0853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f7258-1bd4-4a75-a3cd-85e913919e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def mystery(a: int, b: int) -> int:\n",
    "    \"\"\"Mystery function on two numbers\"\"\"\n",
    "    return a**2 - b**2\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f611c-9c0f-4be0-952a-7580ce14b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [multiply_tool, mystery_tool], llm=llm, verbose=True\n",
    ")\n",
    "agent = agent_worker.as_agent(memory=composable_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6fc63-7330-4e2e-b8a4-25e87c2325b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the mystery function on 5 and 6?\n",
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"a\": 5, \"b\": 6}\n",
      "=== Function Output ===\n",
      "-11\n",
      "=== LLM Response ===\n",
      "The mystery function on 5 and 6 returns -11.\n",
      "assistant: The mystery function on 5 and 6 returns -11.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the mystery function on 5 and 6?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91dbff5-cbc6-49c2-a5af-f30af72e4c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What is the mystery function on 5 and 6?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_ffPgUmNuVavvvDvmHhslNCx2', function=Function(arguments='{\\n  \"a\": 5,\\n  \"b\": 6\\n}', name='mystery'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='-11', additional_kwargs={'name': 'mystery', 'tool_call_id': 'call_ffPgUmNuVavvvDvmHhslNCx2'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The mystery function on 5 and 6 returns -11.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d1205-a0cb-40e5-b037-6ab551ea3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c896d7-8e65-4d16-8307-ac513b5f5230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a73b5-13e5-4927-a1a4-18fbaa843e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was the output of the mystery function on 5 and 6 again? Don't recompute.\n",
      "=== LLM Response ===\n",
      "I'm sorry, but I don't have access to the previous output of the mystery function on 5 and 6.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"What was the output of the mystery function on 5 and 6 again? Don't recompute.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da44b7-c511-4b8b-87a1-d6e36330b41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a helpful assistant.\\n\\nBelow are a set of relevant dialogues retrieved from potentially several memory sources:\\n\\n=====Relevant messages from memory source 1=====\\n\\n\\tUSER: What is the mystery function on 5 and 6?\\n\\tASSISTANT: None\\n\\tTOOL: -11\\n\\tASSISTANT: The mystery function on 5 and 6 returns -11.\\n\\n=====End of relevant messages from memory source 1======\\n\\nThis is the end of the of retrieved message dialogues.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content=\"What was the output of the mystery function on 5 and 6 again? Don't recompute.\", additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\"I'm sorry, but I don't have access to the previous output of the mystery function on 5 and 6.\", additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve from long term\n",
    "agent.memory.get_and_set(\"mystery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c440617-2123-4d50-9244-1f47149d9ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was the output of the mystery function on 5 and 6 again? Don't recompute.\n",
      "=== LLM Response ===\n",
      "The output of the mystery function on 5 and 6 is -11.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"What was the output of the mystery function on 5 and 6 again? Don't recompute.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-core",
   "language": "python",
   "name": "llama-index-core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
