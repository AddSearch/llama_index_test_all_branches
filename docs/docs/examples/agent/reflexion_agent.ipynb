{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e22cff-df9c-4977-bee4-debdb61ee8b7",
   "metadata": {},
   "source": [
    "# Self-Reflection Agent (Reflexion)\n",
    "\n",
    "In this notebook we show you how to create an agent system that can perform \"self-reflection\". Given an agent that can perform auto-retrieval against a RAG pipeline, another reflection agent can critique the outputs, give both a score and detailed feedback. This feedback is injected into the overall memory of the agent. \n",
    "\n",
    "This is inspired by the following papers:\n",
    "- [Reflexion: Language Agents with Verbal Reinforcement Learning, by Shinn et al. (2023)](https://arxiv.org/pdf/2303.11366.pdf)\n",
    "- CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, by Gou et al. (2024)[https://arxiv.org/pdf/2305.11738.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ee89a-7bfc-4a52-b1e4-3ab9f86e3b91",
   "metadata": {},
   "source": [
    "## Setup RAG Pipeline\n",
    "\n",
    "Here we build a RAG pipeline over the Reflexion paper as a data source, and wrap it as a function tool that has both a semantic query argument and filters.\n",
    "\n",
    "This allows us to filter by page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd4d335-99b2-489f-9f2b-4a8a8209c1d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-14 00:01:54--  https://arxiv.org/pdf/2303.11366.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.131.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 591097 (577K) [application/pdf]\n",
      "Saving to: ‘reflexion.pdf’\n",
      "\n",
      "reflexion.pdf       100%[===================>] 577.24K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-04-14 00:01:54 (16.9 MB/s) - ‘reflexion.pdf’ saved [591097/591097]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://arxiv.org/pdf/2303.11366.pdf\" -O reflexion.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c63705-b19f-4b8e-bce3-4bb854c18f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define gpt-3.5-turbo as the initial default\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f33bef1-b5bc-4ceb-829a-84a2ada0e240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "from typing import List, Optional\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"reflexion.pdf\"]).load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Use to answer questions over the paper.\n",
    "\n",
    "    Args:\n",
    "        query (str): the string query to be embedded.\n",
    "        page_numbers (List[str]): Filter by set of pages. Leave blank  \n",
    "            if we want to perform a vector search\n",
    "            over all pages. Otherwise, filter by the set of specified pages.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    page_numbers = page_numbers or []\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "\n",
    "vector_tool = FunctionTool.from_defaults(\n",
    "    name=f\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b905b1b-11c2-427b-9e4a-fc2c5a3456bb",
   "metadata": {},
   "source": [
    "## Build Function Calling Agent \n",
    "\n",
    "Here we define a function that allows us to construct a function-calling agent. \n",
    "\n",
    "This function-calling agent is capable of querying the tool. However as we see it can hallucinate values! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09ade8b5-3473-483c-9473-1aeb56d05c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import call_tool_with_selection\n",
    "from llama_index.core.agent import AgentChatResponse\n",
    "\n",
    "def call_tool( \n",
    "    tools: List[FunctionTool], \n",
    "    chat_history: Optional[List[ChatMessage]] = None,\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> AgentChatResponse:\n",
    "    \"\"\"Simple function to create a RAG agent.\"\"\"\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    chat_history = chat_history or []\n",
    "    if system_prompt is not None:\n",
    "        chat_history = [ChatMessage.from_str(system_prompt, role=\"system\")] + chat_history\n",
    "        \n",
    "    # NOTE: we don't use the higher-level predict_and_call because we want to get both the \n",
    "    # assistant message and the final tool message\n",
    "    response = llm.chat_with_tools(\n",
    "        tools,\n",
    "        chat_history=chat_history,\n",
    "        verbose=True\n",
    "    )\n",
    "    tool_calls = llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n",
    "    if len(tool_calls) == 0:\n",
    "        tool_message = None\n",
    "    else:\n",
    "        tool_output = call_tool_with_selection(tool_calls[0], tools, verbose=True)\n",
    "        # return the assistant message and tool message\n",
    "        tool_message = ChatMessage.from_str(\n",
    "            str(tool_output),\n",
    "            role=\"tool\",\n",
    "            additional_kwargs={\n",
    "                \"name\": tool_calls[0].tool_name,\n",
    "                \"tool_call_id\": tool_calls[0].tool_id,\n",
    "            },\n",
    "        )\n",
    "    return response.message, tool_message\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "796cf0e1-a9a7-45ba-96f5-ba425da4a624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"evaluation dataset\", \"page_numbers\": [\"4\"]}\n",
      "=== Function Output ===\n",
      "The Evaluator component of the Reflexion framework assesses the quality of the generated outputs produced by the Actor. It takes as input a generated trajectory and computes a reward score that reflects its performance within the given task context. The Evaluator model explores various reward functions based on different evaluation criteria, such as exact match grading for reasoning tasks and pre-defined heuristic functions for decision-making tasks. Additionally, different variants of an LLM are used as Evaluators to generate rewards for decision-making and programming tasks. This multi-faceted approach to Evaluator design allows for the examination of different strategies for scoring generated outputs, providing insights into their effectiveness and suitability across a range of tasks.\n"
     ]
    }
   ],
   "source": [
    "user_msg = ChatMessage.from_str(\"Give me more details on the evaluation dataset\", role=\"user\")\n",
    "assistant_msg, tool_msg = call_tool(\n",
    "    [vector_tool],\n",
    "    chat_history=[user_msg]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e401e1e-82a7-4669-960b-c6be12777009",
   "metadata": {},
   "source": [
    "## Build Evaluation Agent\n",
    "\n",
    "In this section we build an evaluation \"agent\" that will take in the current trajectory of the agent, the tool call, the output, and give back feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6949d609-e5e0-470e-b411-4aeb5a31f1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.agent.types import Task, TaskStep, TaskStepOutput\n",
    "from llama_index.core.tools import ToolOutput\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    \"\"\"Reflection of the current agent state.\"\"\"\n",
    "    \n",
    "    is_done: bool = Field(..., description=\"Whether the task is successfully completed according to evaluation criteria (do NOT output True if not).\")\n",
    "    feedback: str = Field(..., description=\"Feedback on how the output can be improved (especially if score is less than 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77bc8ebe-bff7-4b0d-a97d-f5f61b438601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.program.openai import OpenAIPydanticProgram\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "reflection_prompt_str = \"\"\"\n",
    "You are responsible for evaluating whether an agent is taking the right steps towards a solution.\n",
    "\n",
    "You are given the current conversation history, which contains the user task, assistant responses + tool calls, \\\n",
    "as well as any feedback that you have already given.\n",
    "\n",
    "Evaluate the following criteria:\n",
    "- Whether the tool call arguments make sense\n",
    "    - Specifically, check whether page numbers are specified when they shouldn't have. They should ONLY be specified\n",
    "    if in the user query. Do NOT return done if this is the case.\n",
    "- Whether the tool output completes the task.\n",
    "- Whether the final message is an ASSISTANT message (not a tool message). Only if the final message\n",
    "    is an assistant message does it mean the agent is done thinking.\n",
    "\n",
    "Given the current chat history, please output a reflection response in the following format evaluating\n",
    "the quality of the agent trajectory:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "feedback_str_tmpl = \"\"\"\n",
    "Here is a reflection on the current trajectory.\n",
    "\n",
    "{reflection_output}\n",
    "\n",
    "If is_done is not True, there should be feedback on what is going wrong.\n",
    "Given the feedback, please try again.\n",
    "\"\"\"\n",
    "\n",
    "def reflect(chat_history: List[ChatMessage], verbose: bool = False) -> ChatMessage:\n",
    "    \"\"\"Reflect on the trajectory.\"\"\"\n",
    "    \n",
    "    eval_llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)\n",
    "    # print(chat_history)\n",
    "    reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "        message_templates=[\n",
    "            ChatMessage.from_str(reflection_prompt_str, \"system\"),\n",
    "            *chat_history\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        Reflection, \n",
    "        prompt=reflection_prompt,\n",
    "        llm=eval_llm\n",
    "    )\n",
    "    reflection = program()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"> Reflection: {reflection.dict()}\")\n",
    "    \n",
    "    # end state: return user message\n",
    "    reflection_output_str = f\"Is Done: {reflection.is_done}\\nFeedback: {reflection.feedback}\"\n",
    "    feedback_str = feedback_str_tmpl.format(reflection_output=reflection_output_str)\n",
    "\n",
    "    return reflection, ChatMessage.from_str(feedback_str, role=\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "addb6f71-6eef-4276-9317-8c4da68389da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reflection(is_done=True, feedback=\"The assistant's response provides a concise summary of the evaluation dataset as described in the Reflexion framework, focusing on the role of the Evaluator component. However, the response could be improved by clarifying that the information is specific to the Reflexion framework and by providing more context about the framework itself to help the user understand the relevance of the evaluation dataset within that specific context. Additionally, the assistant should avoid specifying page numbers in the tool call unless explicitly requested by the user, as this can limit the breadth of information retrieved.\")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [user_msg, assistant_msg, tool_msg]\n",
    "reflection, reflection_msg = reflect(chat_history)\n",
    "reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46b4ec-4b1f-437e-ade4-a3b6130249f4",
   "metadata": {},
   "source": [
    "## Build Tool Calling Agent with Self-Reflection\n",
    "\n",
    "Now that we have both a tool calling and evaluation module, we can compose a full tool calling agent with reflection built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "70e43a7c-6ba0-4992-a2ba-4a3d7727c096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import (\n",
    "    CustomSimpleAgentWorker,\n",
    "    Task,\n",
    "    AgentChatResponse,\n",
    ")\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "\n",
    "class SelfReflectionAgentWorker(CustomSimpleAgentWorker):\n",
    "    \"\"\"Agent worker that combines tool calling with self-reflection.\n",
    "\n",
    "    Continues iterating until there's no errors / task is done.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    max_iterations: int = Field(default=5)\n",
    "\n",
    "    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize state.\"\"\"\n",
    "        return {\"count\": 0, \"chat_history\": []}\n",
    "\n",
    "    def _run_step(\n",
    "        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n",
    "    ) -> Tuple[AgentChatResponse, bool]:\n",
    "        \"\"\"Run step.\"\"\"\n",
    "        # if first step, add user input\n",
    "        if len(state[\"chat_history\"]) == 0:\n",
    "            state[\"chat_history\"].append(\n",
    "                ChatMessage.from_str(task.input, role=\"user\")\n",
    "            )\n",
    "        \n",
    "        # call tool\n",
    "        assistant_msg, tool_msg = call_tool(self.tools, chat_history=state[\"chat_history\"])\n",
    "        # add assistant message to chat history\n",
    "        state[\"chat_history\"].append(assistant_msg)\n",
    "        # if tool_msg is not None, then also add to chat history\n",
    "        if tool_msg is not None:\n",
    "            state[\"chat_history\"].append(tool_msg)\n",
    "        \n",
    "        # reflect on the current chat history\n",
    "        reflection, reflection_msg = reflect(state[\"chat_history\"], verbose=True)\n",
    "        \n",
    "        # if reflection doesn't indicate completeness, then add feedback as user message\n",
    "        if not reflection.is_done:\n",
    "            state[\"chat_history\"].append(reflection_msg)\n",
    "        \n",
    "        # return response\n",
    "        return AgentChatResponse(response=str(assistant_msg)), reflection.is_done\n",
    "\n",
    "    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:\n",
    "        \"\"\"Finalize task.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00fb7370-0732-4710-88bc-befbabe655f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = SelfReflectionAgentWorker.from_tools(\n",
    "    [vector_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54c1966b-a22b-439d-8cfa-8b637393798e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"evaluation dataset\", \"page_numbers\": [\"4\"]}\n",
      "=== Function Output ===\n",
      "evaluation dataset\n",
      "> Reflection: {'is_done': False, 'feedback': \"The assistant made an incorrect tool call by specifying a page number without it being mentioned in the user query. The assistant should have made a general search without specifying page numbers to find information on the evaluation dataset. Additionally, the assistant failed to provide a response based on the tool's output, which was not informative. The assistant needs to make a correct tool call without specifying page numbers and then provide a detailed response based on the information found.\"}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"evaluation dataset\", \"page_numbers\": []}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the study is HumanEval Python, specifically utilizing starchat-beta for the evaluation. The Pass@1 accuracy results for different models were recorded and compared in the evaluation dataset.\n",
      "> Reflection: {'is_done': True, 'feedback': 'The assistant successfully made a correct tool call without specifying page numbers and provided a detailed response based on the information found about the evaluation dataset.'}\n",
      "assistant: None\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Give me more details on the evaluation dataset?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff3b77-3c03-4ab6-b491-0902c71ddd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
