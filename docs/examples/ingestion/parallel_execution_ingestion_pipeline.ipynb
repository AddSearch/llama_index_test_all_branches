{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1de0f1a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/ingestion/parallel_execution_ingestion_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbe152-de29-4240-8e13-f74dc146a658",
   "metadata": {},
   "source": [
    "# Parallelizing Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd7dec-c846-4ae1-98ab-5436fac08668",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to execute ingestion pipelines using parallel processes. Both sync and async versions of batched parallel execution are possible with `IngestionPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-openai in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (0.1.6)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-embeddings-openai) (0.10.13)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.12.0)\n",
      "Requirement already satisfied: pandas in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.10.14)\n",
      "Requirement already satisfied: anyio in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.14.0)\n",
      "Requirement already satisfied: click in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0963707-6ebe-4441-a363-1bfb48ce9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4bcbf-491d-4d55-bade-a40d5e8b32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats\n",
    "from pstats import SortKey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba575e-2635-4598-a74a-d4036c1816db",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92686bb0-85ed-4bb3-99eb-f5fc6c100787",
   "metadata": {},
   "source": [
    "For this notebook, we'll load the `PatronusAIFinanceBenchDataset` llama-dataset from [llamahub](https://llamahub.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b94d62-efa4-479a-9215-e094b5a73061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:25<00:00,  1.26it/s]\n",
      "Successfully downloaded PatronusAIFinanceBenchDataset to ./data\n"
     ]
    }
   ],
   "source": [
    "!llamaindex-cli download-llamadataset PatronusAIFinanceBenchDataset --download-dir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f7e5b-6430-426b-b239-e9280ea7b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data/source_files\").load_data(\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00be91-22ea-403c-b9c4-cd030b7e6c09",
   "metadata": {},
   "source": [
    "### Define our IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089adee-bc8a-457f-8d96-113435923d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n",
    "        # TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# since we'll be testing performance, using timeit and cProfile\n",
    "# we're going to disable cache\n",
    "pipeline.disable_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1937fbaa-0cef-494d-b3e1-a5ff268fd8d2",
   "metadata": {},
   "source": [
    "### Parallel Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20d688-5994-4cd7-8f52-079b686328fb",
   "metadata": {},
   "source": [
    "A single run. Setting `num_workers` to a value greater than 1 will invoke parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d68e6a-a658-46e8-9b71-d857b1c90d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627d411-8fad-43ca-a6f0-533635e0c613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffaefb-f710-4187-a19f-11d10ebae82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.9 s ± 732 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pipeline.run(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d514fe-313b-4b88-8122-c5a44db210df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  4 22:58:56 2024    newstats\n",
      "\n",
      "         2050 function calls in 25.435 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 215 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   25.435   25.435 {built-in method builtins.exec}\n",
      "        1    0.049    0.049   25.434   25.434 <string>:1(<module>)\n",
      "        1    0.000    0.000   25.386   25.386 pipeline.py:665(run)\n",
      "       12    0.000    0.000   25.327    2.111 threading.py:589(wait)\n",
      "       11    0.000    0.000   25.327    2.302 threading.py:288(wait)\n",
      "       71   25.327    0.357   25.327    0.357 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000   25.324   25.324 pool.py:369(starmap)\n",
      "        1    0.000    0.000   25.324   25.324 pool.py:767(get)\n",
      "        1    0.000    0.000   25.324   25.324 pool.py:764(wait)\n",
      "        1    0.000    0.000    0.049    0.049 context.py:115(Pool)\n",
      "        1    0.000    0.000    0.049    0.049 pool.py:183(__init__)\n",
      "        1    0.000    0.000    0.045    0.045 pool.py:305(_repopulate_pool)\n",
      "        1    0.000    0.000    0.045    0.045 pool.py:314(_repopulate_pool_static)\n",
      "        4    0.000    0.000    0.044    0.011 process.py:110(start)\n",
      "        4    0.000    0.000    0.044    0.011 context.py:285(_Popen)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x2a360f9d0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run(\n",
    "    \"pipeline.run(documents=documents, num_workers=4)\",\n",
    "    \"newstats\",\n",
    ")\n",
    "p = pstats.Stats(\"newstats\")\n",
    "p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482139e-1d0b-41ac-bff0-0c4a86a3ce62",
   "metadata": {},
   "source": [
    "### Async Parallel Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e3ede-1ff4-430c-abfb-270be055ff71",
   "metadata": {},
   "source": [
    "Here the `ProcessPoolExecutor` from `concurrent.futures` is used to execute processes asynchronously. The tasks are being processed are blocking, but also performed asynchronously on the individual processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7856e-66ee-44ac-94c6-85082d75d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = await pipeline.arun(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c980287-082e-4b0f-b85d-ee3362400eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0bf6c-510c-44b3-b9f6-570593321817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.2501484448736715 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9942046, Requested 74706. Please try again in 100ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.4651037530906399 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9952471, Requested 74238. Please try again in 160ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.8573717519750571 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9943645, Requested 81947. Please try again in 153ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.10944232541253651 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9997418, Requested 86576. Please try again in 503ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.3154476073680167 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9941856, Requested 83917. Please try again in 154ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.28682685646345274 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9995108, Requested 69758. Please try again in 389ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.3220542031151039 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9940382, Requested 74238. Please try again in 87ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.7170649903010743 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9933220, Requested 82545. Please try again in 94ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embeddings in 0.2449392439520195 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-1ZDAvajC6v2ZtAP9hLEIsXRz on tokens per min (TPM): Limit 10000000, Used 9917797, Requested 83787. Please try again in 9ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 s ± 4.38 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "%timeit loop.run_until_complete(pipeline.arun(documents=documents, num_workers=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29209f1e-56a2-4d39-b983-59121a6f1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  4 23:02:02 2024    async-newstats\n",
      "\n",
      "         2730 function calls in 13.905 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 286 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   13.905   13.905 {built-in method builtins.exec}\n",
      "        1    0.051    0.051   13.905   13.905 <string>:1(<module>)\n",
      "        1    0.000    0.000   13.853   13.853 nest_asyncio.py:86(run_until_complete)\n",
      "       13    0.000    0.000   13.853    1.066 nest_asyncio.py:100(_run_once)\n",
      "       13    0.000    0.000   13.561    1.043 selectors.py:554(select)\n",
      "       13   13.561    1.043   13.561    1.043 {method 'control' of 'select.kqueue' objects}\n",
      "       26    0.000    0.000    0.292    0.011 events.py:78(_run)\n",
      "       26    0.000    0.000    0.292    0.011 {method 'run' of '_contextvars.Context' objects}\n",
      "        2    0.000    0.000    0.291    0.145 tasks.py:215(__step)\n",
      "        2    0.000    0.000    0.291    0.145 {method 'send' of 'coroutine' objects}\n",
      "        2    0.000    0.000    0.291    0.145 pipeline.py:835(arun)\n",
      "       66    0.244    0.004    0.244    0.004 {method 'acquire' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.232    0.232 tasks.py:302(__wakeup)\n",
      "        1    0.000    0.000    0.232    0.232 _base.py:648(__exit__)\n",
      "        1    0.000    0.000    0.232    0.232 process.py:771(shutdown)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x12e943eb0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "cProfile.run(\n",
    "    \"loop.run_until_complete(pipeline.arun(documents=documents, num_workers=4))\",\n",
    "    \"async-newstats\",\n",
    ")\n",
    "p = pstats.Stats(\"async-newstats\")\n",
    "p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e345d8-0524-4e1b-8d11-88a2a916196e",
   "metadata": {},
   "source": [
    "### Sequential Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80091185-d7ac-4ff2-aba4-e1ba5546a865",
   "metadata": {},
   "source": [
    "By default `num_workers` is set to `None` and this will invoke sequential execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31aabf-da4d-4a4a-b92c-2b83a75b296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a116fd0-829e-4138-8461-ee4da5708f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8b9c1-9129-43e6-9d7d-cd50b3abc953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 11s ± 3.56 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf08074-3bb1-46bb-86f0-aca8e103e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  4 23:14:42 2024    oldstats\n",
      "\n",
      "         5713181 function calls (5496694 primitive calls) in 68.806 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 714 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   68.810   68.810 {built-in method builtins.exec}\n",
      "        1    0.053    0.053   68.810   68.810 <string>:1(<module>)\n",
      "        1    0.000    0.000   68.757   68.757 pipeline.py:665(run)\n",
      "        1    0.015    0.015   68.757   68.757 pipeline.py:97(run_transformations)\n",
      "        1    5.145    5.145   63.002   63.002 base.py:333(__call__)\n",
      "        1    0.010    0.010   57.827   57.827 base.py:233(get_text_embedding_batch)\n",
      "       54    0.000    0.000   57.807    1.070 base.py:411(_get_text_embeddings)\n",
      "       54    0.000    0.000   57.806    1.070 __init__.py:287(wrapped_f)\n",
      "       54    0.003    0.000   57.806    1.070 __init__.py:369(__call__)\n",
      "       54    0.001    0.000   57.799    1.070 base.py:163(get_embeddings)\n",
      "       54    0.001    0.000   57.784    1.070 embeddings.py:35(create)\n",
      "       54    0.000    0.000   57.519    1.065 _base_client.py:1186(post)\n",
      "       54    0.000    0.000   57.514    1.065 _base_client.py:880(request)\n",
      "       54    0.001    0.000   57.514    1.065 _base_client.py:897(_request)\n",
      "       54    0.000    0.000   56.464    1.046 _client.py:881(send)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10ea61cf0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run(\"pipeline.run(documents=documents)\", \"oldstats\")\n",
    "p = pstats.Stats(\"oldstats\")\n",
    "p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404fef3-ea1c-4b38-a558-c5be27bdd9f7",
   "metadata": {},
   "source": [
    "### Async on Main Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b8e77-199c-4afc-870d-91fafc112f8e",
   "metadata": {},
   "source": [
    "As with the sync case, `num_workers` is default to `None`, which will then lead to single-batch execution of async tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca073ac-ed85-4d29-821e-2acf37ea5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = await pipeline.arun(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173231f-bea7-49b6-895c-25ac4aa352b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37efa7-3936-4cf8-a029-fcba95205218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 s ± 3.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loop.run_until_complete(pipeline.arun(documents=documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54d99c-0ea9-46cb-8b4e-9fd97ef2b7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar  4 23:18:06 2024    async-oldstats\n",
      "\n",
      "         6901892 function calls (6678831 primitive calls) in 15.789 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1015 to 15 due to restriction <15>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   15.799   15.799 {built-in method builtins.exec}\n",
      "        1    0.043    0.043   15.799   15.799 <string>:1(<module>)\n",
      "        1    0.002    0.002   15.756   15.756 nest_asyncio.py:86(run_until_complete)\n",
      "     2078    0.035    0.000   15.753    0.008 nest_asyncio.py:100(_run_once)\n",
      "     5995    0.004    0.000   12.996    0.002 events.py:78(_run)\n",
      "     5995    0.003    0.000   12.992    0.002 {method 'run' of '_contextvars.Context' objects}\n",
      "     3127    0.010    0.000   12.916    0.004 tasks.py:215(__step)\n",
      "     3072    0.002    0.000   12.893    0.004 {method 'send' of 'coroutine' objects}\n",
      "        2    0.000    0.000   11.321    5.660 pipeline.py:835(arun)\n",
      "        2    0.000    0.000   11.321    5.660 pipeline.py:132(arun_transformations)\n",
      "        1    0.000    0.000    6.230    6.230 schema.py:129(acall)\n",
      "        1    0.000    0.000    6.230    6.230 interface.py:114(__call__)\n",
      "        1    0.042    0.042    6.230    6.230 interface.py:57(get_nodes_from_documents)\n",
      "     1378    0.001    0.000    5.751    0.004 tasks.py:302(__wakeup)\n",
      "        1    0.008    0.008    5.701    5.701 interface.py:171(_parse_nodes)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x2ba2e8190>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run(\n",
    "    \"loop.run_until_complete(pipeline.arun(documents=documents))\",\n",
    "    \"async-oldstats\",\n",
    ")\n",
    "p = pstats.Stats(\"async-oldstats\")\n",
    "p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bfb79-2def-462c-b3bb-d446e3bb9463",
   "metadata": {},
   "source": [
    "### In Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e90d6-013a-43d6-8c49-dbd78709587a",
   "metadata": {},
   "source": [
    "The results from the above experiments are re-shared below where each strategy is listed from fastest to slowest with this example dataset and pipeline.\n",
    "\n",
    "1. (Async, Parallel Processing): 20.3s \n",
    "2. (Async, No Parallel Processing): 20.5s\n",
    "3. (Sync, Parallel Processing): 29s\n",
    "4. (Sync, No Parallel Processing): 1min 11s\n",
    "\n",
    "We can see that both cases that use Parallel Processing outperforms the Sync, No Parallel Processing (i.e., `.run(num_workers=None)`). Also, that at least for this case for Async tasks, there is little gains in using Parallel Processing. Perhaps for larger workloads and IngestionPipelines, using Async with Parallel Processing can lead to larger gains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
